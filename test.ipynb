{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.covariance as cor\n",
    "\n",
    "import sklearn.svm as svm\n",
    "from sklearn.metrics import classification_report as clf_rpt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import metrics  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# os.chdir('/Users/caojiawu/test')\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "#doAgeNull = 0, remove NaN; doAgeNull = 1, Set random value; doAgeNull = 2, Set predicted value;\n",
    "doAgeNull = 2\n",
    "doDummyVar = 1\n",
    "\n",
    "#doSampleEven = 1, Keep samples of Survived to be even.  doSampleEven = 0, do nothing.\n",
    "doSampleEven = 0 \n",
    "\n",
    "# Multinomial Naive Bayes Classifier    \n",
    "def naive_bayes_classifier(train_x, train_y):     \n",
    "    from sklearn.naive_bayes import MultinomialNB    \n",
    "    model = MultinomialNB(alpha=0.01)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "   \n",
    "    \n",
    "# KNN Classifier    \n",
    "def knn_classifier(train_x, train_y):    \n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    model = KNeighborsClassifier()    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# Logistic Regression Classifier    \n",
    "def logistic_regression_classifier(train_x, train_y):    \n",
    "    from sklearn.linear_model import LogisticRegression    \n",
    "    model = LogisticRegression(penalty='l2')    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "   \n",
    "    \n",
    "# Random Forest Classifier    \n",
    "def random_forest_classifier(train_x, train_y):    \n",
    "    from sklearn.ensemble import RandomForestClassifier    \n",
    "    model = RandomForestClassifier(n_estimators=8)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# Decision Tree Classifier    \n",
    "def decision_tree_classifier(train_x, train_y):    \n",
    "    from sklearn import tree    \n",
    "    model = tree.DecisionTreeClassifier()    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier    \n",
    "def gradient_boosting_classifier(train_x, train_y):    \n",
    "    from sklearn.ensemble import GradientBoostingClassifier    \n",
    "    model = GradientBoostingClassifier(n_estimators=200)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# SVM Classifier    \n",
    "def svm_classifier(train_x, train_y):    \n",
    "    from sklearn.svm import SVC    \n",
    "    model = SVC(kernel='rbf', probability=True)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "# SVM Classifier using cross validation    \n",
    "def svm_cross_validation(train_x, train_y):    \n",
    "    from sklearn.grid_search import GridSearchCV    \n",
    "    from sklearn.svm import SVC    \n",
    "    model = SVC(kernel='rbf', probability=True)    \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}    \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)    \n",
    "    grid_search.fit(train_x, train_y)    \n",
    "    best_parameters = grid_search.best_estimator_.get_params()    \n",
    "    for para, val in list(best_parameters.items()):    \n",
    "        print(para, val)    \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把每个年龄存活均值打印出来。需要注意的是部分年龄没有人和没有存活的人是不同的,但图中都显示的是0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd428c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = data[data['Age'].notnull()][['Survived','Age']]\n",
    "a['Age'] = a['Age'].astype('int')\n",
    "b=a.groupby('Age').mean()\n",
    "plt.bar(range(len(b)),b['Survived'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗，数据补全\n",
    "对于可以利用的数据，如果数据是NaN，可以利用已有数据的统计特征进行最小误差引入的补全.\n",
    "\n",
    "将性别转换为数值型：female=0，male=1.\n",
    "\n",
    "Embarked取值没有连续性，因此将Embarked做哑变量处理，变成三个特征量：EmbarkedS,EmbarkedC,EmbarkedQ,取值为开关量[0,1]\n",
    "\n",
    "Pclass同样做哑变量处理，变成三个特征量，Pclass1,Pclass2,Pclass3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tempData = data\n",
    "\n",
    "#let female=0 and male=1\n",
    "tempData['Sex'] = tempData['Sex'].apply(lambda x: 0 if x=='female' else 1)\n",
    "\n",
    "if doDummyVar == 1:\n",
    "    tempData['Pclass1']=tempData['Pclass'].apply(lambda x: 1 if x==1 else 0)\n",
    "    tempData['Pclass2']=tempData['Pclass'].apply(lambda x: 1 if x==2 else 0)\n",
    "    tempData['Pclass3']=tempData['Pclass'].apply(lambda x: 1 if x==3 else 0)\n",
    "#     tempData = tempData.drop('Pclass',1)\n",
    "\n",
    "    tempData['EmbarkedC']=tempData['Embarked'].apply(lambda x: 1 if x=='C' else 0)\n",
    "    tempData['EmbarkedQ']=tempData['Embarked'].apply(lambda x: 1 if x=='Q' else 0)\n",
    "    tempData['EmbarkedS']=tempData['Embarked'].apply(lambda x: 1 if x=='S' else 0)\n",
    "#     tempData = tempData.drop('Embarked',1)\n",
    "\n",
    "tempData['Embarked'] = tempData['Embarked'].map({'C':0,'Q':1,'S':2})\n",
    "#     tempData['Embarked'] = tempData['Embarked'].map({'C':2,'Q':1,'S':0})\n",
    "#     tempData['Embarked'] = tempData['Embarked'].apply(lambda x: 0 if x=='C' else 1 if x=='Q' else 2)\n",
    "tempData = tempData.drop('PassengerId',1)\n",
    "tempData = tempData.drop('Name',1)\n",
    "tempData = tempData.drop('Cabin',1)\n",
    "tempData = tempData.drop('Ticket',1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【测试一】 NaN参数处理测试\n",
    "\n",
    "几种对NaN参数处理的对比：\n",
    "- 直接丢弃包含NaN参数的行\n",
    "\n",
    "直接丢弃会造成样本数减少，损失了有用信息。\n",
    "- 生成随机数填充NaN特征\n",
    "\n",
    "下面的例子用高斯分布（Age均值，Age方差）去生成数据填充NaN列\n",
    "- 利用其他有效特征去预测NaN列的值\n",
    "\n",
    "此例中Age列是有价值的特征。利用其他列的值，选择可能和年龄相关的几列'Survived','SibSp','Fare','Pclass'预测Age列的值。因Age是连续值，此处用到的是回归算法。\n",
    "<font color=#ff0000 size=4>注：网上有帖子提到，对于回归模型，根据情况需要做哑变量处理，理由是涉及类似欧式距离的计算无须离散类型值是有问题的。但实际测试下来没发现哑变量会改善模型准确性，需要研究各算法的特点和分析原因。</font>\n",
    "\n",
    "验证的时候需要注意，预测Age用的训练数据随机选择时，可能会由于选择的训练集合不同得到不同的结果，因此需要多做几个不同的random_state选择，然后与其他补NaN的测试结果对比。\n",
    "做了几次测试后，预测Age值的手段会比随机补NaN的准确性要好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.334077023871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if doAgeNull == 2:\n",
    "    tempData_2 = tempData.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "    \n",
    "    if doDummyVar == 1:\n",
    "        X_train, X_test,y_train,y_test = train_test_split(\n",
    "#             tempData_2[['Sex','Survived','SibSp','Fare','Pclass1','Pclass2','Pclass3','EmbarkedC','EmbarkedQ','EmbarkedS']],\n",
    "            tempData_2[['Survived','SibSp','Fare','Pclass1','Pclass2','Pclass3']],\n",
    "            tempData_2['Age'],test_size=0.25,random_state=27)\n",
    "    else:\n",
    "        X_train, X_test,y_train,y_test = train_test_split(\n",
    "            tempData_2[['Survived','SibSp','Fare','Pclass']],\n",
    "            tempData_2['Age'],test_size=0.25,random_state=27)        \n",
    "\n",
    "#     model = LinearRegression()\n",
    "#     model = SVR(C=1024,gamma=0.5)\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    u = model.score(X_test,y_test)\n",
    "    print(u)\n",
    "\n",
    "    m = tempData[tempData['Age'].isnull()]\n",
    "    if doDummyVar == 1:\n",
    "        v = model.predict(m[['Survived','SibSp','Fare','Pclass1','Pclass2','Pclass3']])\n",
    "    else:\n",
    "        v = model.predict(m[['Survived','SibSp','Fare','Pclass']])\n",
    "\n",
    "    tempData['Age'][tempData['Age'].isnull()] = v\n",
    "elif doAgeNull == 1:\n",
    "    meanAge = tempData['Age'].mean()\n",
    "    print(meanAge)\n",
    "    m = tempData['Age'].mean()\n",
    "    s = tempData['Age'].std()\n",
    "    np.random.seed(0)\n",
    "    tempData['Age'][tempData['Age'].isnull()] = tempData[tempData['Age'].isnull()]['Age'].apply(lambda x: np.random.normal(m,s,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 由于对样本做随机划分得到的训练级和测试集训练后的score表现为随机性，使用K折交叉验证，然后观察多次训练模型准确性的平均值。\n",
    "\n",
    "# 【测试二】 Survived=1的样本少于Survived=0的样本的，做样本均衡\n",
    "由于不能直接在样本split前作均衡，否则会把随机复制的样本代入到测试集合中。因此，对于split后的训练样本中Survived=1样本做随机复制，使得其样本数与Survived=0的样本保持相当。\n",
    "<font color=#ff0000 size=4>测试结果显示，采用上述样本均衡方法不能改善总体准确率，但可以改善查准率，查全率会恶化。</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* Multinomial_Naive_Bayes ********************\n",
      "cross validation precision: 72.29%, recall: 54.55%\n",
      "cross validation accuracy: 75.17%\n",
      "mean of K-Folds scores:0.739322598862\n",
      "K-Folds scores:[ 0.62416107  0.74496644  0.73825503  0.7027027   0.82993197  0.79591837]\n",
      "******************* KNN ********************\n",
      "cross validation precision: 74.77%, recall: 72.73%\n",
      "cross validation accuracy: 80.61%\n",
      "mean of K-Folds scores:0.81666241985\n",
      "K-Folds scores:[ 0.75838926  0.83221477  0.83221477  0.85810811  0.80272109  0.81632653]\n",
      "******************* LR ********************\n",
      "cross validation precision: 74.31%, recall: 73.64%\n",
      "cross validation accuracy: 80.61%\n",
      "mean of K-Folds scores:0.8042979118\n",
      "K-Folds scores:[ 0.77181208  0.81208054  0.83221477  0.77702703  0.80272109  0.82993197]\n",
      "******************* RF ********************\n",
      "cross validation precision: 75.24%, recall: 71.82%\n",
      "cross validation accuracy: 80.61%\n",
      "mean of K-Folds scores:0.813329800147\n",
      "K-Folds scores:[ 0.77181208  0.82550336  0.81208054  0.82432432  0.82312925  0.82312925]\n",
      "******************* DT ********************\n",
      "cross validation precision: 66.67%, recall: 74.55%\n",
      "cross validation accuracy: 76.53%\n",
      "mean of K-Folds scores:0.798689440708\n",
      "K-Folds scores:[ 0.75838926  0.82550336  0.77852349  0.83108108  0.78231293  0.81632653]\n",
      "******************* SVM ********************\n",
      "cross validation precision: 72.73%, recall: 72.73%\n",
      "cross validation accuracy: 79.59%\n",
      "mean of K-Folds scores:0.78618786229\n",
      "K-Folds scores:[ 0.79865772  0.77852349  0.83221477  0.76351351  0.76190476  0.78231293]\n",
      "******************* GBDT ********************\n",
      "cross validation precision: 81.00%, recall: 73.64%\n",
      "cross validation accuracy: 83.67%\n",
      "mean of K-Folds scores:0.842572112586\n",
      "K-Folds scores:[ 0.83892617  0.82550336  0.83221477  0.85810811  0.82993197  0.8707483 ]\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.5    \n",
    "\n",
    "tempData = tempData.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "tempData['Age']= tempData['Age'].apply(lambda x: (x-tempData.min()['Age'])/(tempData.max()['Age']-tempData.min()['Age']))\n",
    "tempData['Fare'] = tempData['Fare'].apply(lambda x: (x-tempData.min()['Fare'])/(tempData.max()['Fare']-tempData.min()['Fare']))\n",
    "tempData['SibSp'] = tempData['SibSp'].apply(lambda x: (x-tempData.min()['SibSp'])/(tempData.max()['SibSp']-tempData.min()['SibSp']))\n",
    "tempData['Parch'] = tempData['Parch'].apply(lambda x: (x-tempData.min()['Parch'])/(tempData.max()['Parch']-tempData.min()['Parch']))\n",
    "\n",
    "if doDummyVar == 1:\n",
    "    train_x, test_x,train_y,test_y = train_test_split(\n",
    "            tempData[['Sex','Age','SibSp','Fare','Pclass1','Pclass2','Pclass3','EmbarkedC','EmbarkedQ','EmbarkedS']],\n",
    "            tempData['Survived'],test_size=0.33,random_state=42)\n",
    "else:\n",
    "    train_x, test_x,train_y,test_y = train_test_split(\n",
    "            tempData[['Sex','Age','SibSp','Fare','Pclass','Embarked']],\n",
    "            tempData['Survived'],test_size=0.33,random_state=42)      \n",
    "    \n",
    "if doSampleEven ==1:\n",
    "    count_0 = len(train_x)-train_y.sum()\n",
    "    count_1 = len(train_x) - count_0\n",
    "    train_x['Survived'] = train_y\n",
    "    sampled = train_x[train_x['Survived'] == 1].sample(count_0-count_1)\n",
    "    train_x = train_x.append(sampled)\n",
    "    train_y = train_x['Survived']\n",
    "    train_x = train_x.drop('Survived')\n",
    "    print('dosample')\n",
    "    \n",
    "    \n",
    "# train_x =  train_x.astype('float64')\n",
    "# test_x =  test_x.astype('float64')\n",
    "     \n",
    "test_classifiers = ['Multinomial_Naive_Bayes', 'KNN', 'LR', 'RF', 'DT', 'SVM', 'GBDT']    \n",
    "classifiers = {'Multinomial_Naive_Bayes':naive_bayes_classifier,     \n",
    "                  'KNN':knn_classifier,    \n",
    "                   'LR':logistic_regression_classifier,    \n",
    "                   'RF':random_forest_classifier,    \n",
    "                   'DT':decision_tree_classifier,    \n",
    "                  'SVM':svm_classifier,    \n",
    "                 'GBDT':gradient_boosting_classifier    \n",
    "}            \n",
    "\n",
    "accu = []\n",
    "for classifier in test_classifiers:    \n",
    "    print('******************* %s ********************' % classifier)    \n",
    "    model = classifiers[classifier](train_x, train_y)    \n",
    "    predict = model.predict(test_x)    \n",
    "    precision = metrics.precision_score(test_y, predict)    \n",
    "    recall = metrics.recall_score(test_y, predict)    \n",
    "    print('cross validation precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "    accuracy = metrics.accuracy_score(test_y, predict)    \n",
    "    print('cross validation accuracy: %.2f%%' % (100 * accuracy))\n",
    "    accu.append(accuracy)\n",
    "    \n",
    "\n",
    "\n",
    "    if doDummyVar == 1:\n",
    "        scores = cross_val_score(model, \n",
    "                                 tempData[['Sex','Age','SibSp','Fare','Pclass1','Pclass2','Pclass3','EmbarkedC','EmbarkedQ','EmbarkedS']],\n",
    "                                 tempData['Survived'],cv=6)\n",
    "    else:\n",
    "        scores = cross_val_score(model, tempData[['Sex','Age','SibSp','Fare','Pclass','Embarked']],tempData['Survived'],cv=6)\n",
    "       \n",
    "    print('mean of K-Folds scores:'+str(sum(scores)/len(scores)))\n",
    "    print('K-Folds scores:'+str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
